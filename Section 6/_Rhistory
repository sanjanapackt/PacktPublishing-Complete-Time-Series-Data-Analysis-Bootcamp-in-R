library(mxnet)
data_train = data.matrix(training)
data_train.x = data_train[,-1] #predictors
data_train.x = t(data_train.x/255) #image scaling
data_train.y = data_train[,1]#response var - label
data_test = data.matrix(testing)
data_test.x = data_test[,-1] #predictors
data_test.x = t(data_test.x/255) #image scaling
data_test.y = data_test[,1]#response var - label
data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128)
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128)
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")
fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64)
act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu")
fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10)
softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm")
devices <- mx.cpu() #use cpu
mx.set.seed(42)
model_dnn = mx.model.FeedForward.create(softmax, X=data_train.x,
y=data_train.y, ctx=devices, num.round=30, array.batch.size=100,
learning.rate=0.01, momentum=0.9, eval.metric=mx.metric.accuracy,
initializer=mx.init.uniform(0.1),
epoch.end.callback=mx.callback.log.train.metric(100))
graph.viz(model_dnn$symbol)
prob_dnn <- predict(model_dnn, data_test.x)
prediction_dnn <- max.col(t(prob_dnn)) - 1
cm_dnn = table(testing$label, prediction_dnn)
cm_dnn
accuracy_dnn = mean(prediction_dnn == testing$label)
accuracy_dnn
example_cat_image = readImage(file.path(image_dir, "cat.1.jpg"))
display(example_cat_image)
example_dog_image = readImage(file.path(image_dir, "dog.1.jpg"))
display(example_dog_image)
head(cats_data)
setwd("F:\\SIBERIA\\Papers\\manuscript\\versions\\May2\\vi-only")
p=read.csv("Species_dNBR_Terrain2.csv")
head(p)
names(p)
nameList=c("Density","Percent_Aspen","Percent_Pine","Percent_Larch","Percent_Birch",
"dNBR","NDVI_pre",
"EVI_BurnYear","NDVI_BurnYear","NDMI_BurnYear",
"EVI_2016","NDVI_2016","NDMI_2016","CosineAspect","slope","elevation")
df1=p[,colnames(p) %in% nameList]
head(df1)
df1$dNBR=as.numeric(df1$dNBR)
df1$elevation=as.numeric(df1$elevation)
r=subset(p,State=="R")
nr=subset(p,State=="NR")
df2$elevation=as.numeric(df2$elevation)
df3=r[,colnames(r) %in% nameList]#r
nameList=c("Density","Percent_Aspen","Percent_Pine","Percent_Larch","Percent_Birch",
"dNBR","NDVI_pre",
"EVI_BurnYear","NDVI_BurnYear","NDMI_BurnYear",
"EVI_2016","NDVI_2016","NDMI_2016","CosineAspect","slope","elevation")
df1=p[,colnames(p) %in% nameList]
head(df1)
df1$dNBR=as.numeric(df1$dNBR)
df1$elevation=as.numeric(df1$elevation)
r=subset(p,State=="R")
nr=subset(p,State=="NR")
df2=nr[,colnames(nr) %in% nameList]#nr
df2$dNBR=as.numeric(df2$dNBR)
df2$elevation=as.numeric(df2$elevation)
df3=r[,colnames(r) %in% nameList]#r
df3$dNBR=as.numeric(df3$dNBR)
df3$elevation=as.numeric(df3$elevation)
library(caret)
library(FSelector)
library(FSelector)
library(NeuralNetTools)
library(neuralnet)
normal=function(x){
return((x-min(x))/(max(x)-min(x)))}
con_norm=as.data.frame(lapply(df1,normal))
head(con_norm)#
con_norm2=as.data.frame(lapply(df2,normal))#nr
head(con_norm2)
con_norm3=as.data.frame(lapply(df3,normal))#R
head(con_norm3)
set.seed(1)
nr_regen = neuralnet(formula = Density ~ CosineAspect+slope+elevation+dNBR+NDVI_pre+
EVI_BurnYear + NDMI_BurnYear+
NDVI_BurnYear+EVI_2016+NDVI_2016+NDMI_2016,
data = con_norm2,hidden=5)
olden(nr_regen,main="NR")
set.seed(2)
r_l = neuralnet(formula = Percent_Larch ~ CosineAspect+slope+elevation+dNBR+NDVI_pre+
EVI_BurnYear + NDMI_BurnYear+
NDVI_BurnYear+EVI_2016+NDVI_2016+NDMI_2016,
data = con_norm3,hidden=5)
olden(r_l)
set.seed(3)
r_l = neuralnet(formula = Percent_Larch ~ CosineAspect+slope+elevation+dNBR+NDVI_pre+
EVI_BurnYear + NDMI_BurnYear+
NDVI_BurnYear+EVI_2016+NDVI_2016+NDMI_2016,
data = con_norm3,hidden=5)
olden(r_l)
library(digest)
setwd("F:\\SIBERIA\\Papers\\manuscript\\versions\\May2\\vi-only")
p=read.csv("regen_burn.csv")
head(p)
wilcox.test(p$Density~p$State)
library(ggplot2)
g1= ggplot(p, aes(x=State, y=Density,fill=State)) +
geom_boxplot()+
labs(title="Regeneration Density of R & NR Sites",x="State", y = "Regeneration Density (thousands/hectare)")
g1 + scale_y_continuous(limits = c(0, 55))
require(dplyr)
dfx=p %>%
group_by(State) %>%
summarise(mn=mean(Density, na.rm = TRUE),
std=sd(Density, na.rm = TRUE))
dfx
dfx=p %>%
group_by(State) %>%
summarise(avg=mean(Density, na.rm = TRUE),
std=sd(Density, na.rm = TRUE))
dfx
dfb=p %>%
group_by(State) %>%
summarise(avg=mean(dNBR, na.rm = TRUE),
std=sd(dNBR, na.rm = TRUE))
dfb
dfb=p %>%
group_by(State) %>%
summarise(avg=mean(dNBR, na.rm = TRUE),
std=sd(dNBR, na.rm = TRUE))
dfb
require(dplyr)
dfx=p %>%
group_by(State) %>%
summarise(avg=mean(Density, na.rm = TRUE),
std=sd(Density, na.rm = TRUE))
dfx
dfb=p %>%
group_by(State) %>%
summarise(avg=mean(dNBR, na.rm = TRUE),
std=sd(dNBR, na.rm = TRUE))
dfb
dfb=p %>%
group_by(State) %>%
summarise(avg=mean(dNBR),
std=sd(dNBR))
dfb
wilcox.test(p$Density~p$State)
wilcox.test(p$Percent_Aspen~p$State)
wilcox.test(p$Percent_Birch~p$State)
wilcox.test(p$Percent_Larch~p$State)
wilcox.test(p$Percent_Pine~p$State)
p<-df[,c(2,4:7)] #Select variables
d<-p[,c(2,4:7)] #Select variables
head(d)
library(reshape)
mdata <- melt(d, id= "State")
library(data.table)
install.packages("data.table")
library(data.table)
setnames(mdata, old = c('variable','value'), new = c('species','percent'))
d2<-mdata[!(mdata$percent=="" | mdata$percent==0),]
d3<-aggregate(. ~State+species, data=d2, sum, na.rm=TRUE)
d4<-cast(d3, species~State, sum, value = 'percent')
d4$Pct_NR <- d4$NR / sum(d4$NR)*100
d4$Pct_R <- d4$R / sum(d4$R)*100
d4
d2<-mdata[!(mdata$percent=="" | mdata$percent==0),]
d3<-aggregate(. ~State+species, data=d2, sum, na.rm=TRUE)
d4<-cast(d3, species~State, sum, value = 'percent')
d4$Pct_NR <- d4$NR / sum(d4$NR)*100
d4$Pct_R <- d4$R / sum(d4$R)*100
m<-as.data.frame(d4[,c(1,4:5)]) #Select variables #select
d <- melt(m, id= "species")
setnames(d, old = c('species','variable','value'), new = c('Species','SpeciesComposition','Percent'))
data <- tapply(d$Percent, list(d$SpeciesComposition,d$Species), sum)
barplot(data,beside=T,col=c("#ee7700","#3333ff"),ylim = c(0,80)
,main="Post Fire Tree Species",xlab="Species",ylab="Percentage")
legend("topright", inset=0.05, title="Species Composition",legend = c("Non-Recruiting plots","Recruiting plots"),
fill=c("#ee7700","#3333ff"))
barplot(data,beside=T,col=c("#ee7700","#3333ff"),ylim = c(0,80)
,main="Post Fire Tree Species",xlab="Species",ylab="Percentage")
library("rattle.data")
install.packages("rattle.data")
library("rattle.data")
library(rnn)
data(weatherAUS)
head(weatherAUS)
data=weatherAUS[1:3040,c(1,14)]
summary(data)
data_cleaned <- na.omit(data)
data_used=data_cleaned[1:3000]
x=data_cleaned[,1]
y=data_cleaned[,2]
head(x)
head(y)
X=matrix(x, nrow = 30)
Y=matrix(y, nrow = 30)
Yscaled = (Y - min(Y)) / (max(Y) - min(Y))
Y=t(Yscaled)
train=1:70
test=71:100
model <- trainr(Y = Y[train,],
X = Y[train,],
learningrate = 0.05,
hidden_dim = 16,
numepochs = 1000)
plot(colMeans(model$error),type='l',xlab='epoch',ylab='errors')
Yp <- predictr(model, Y[test,])
plot(as.vector(t(Y[test,])), col = 'red', type='l',
main = "Actual vs Predicted Humidity: testing set",
ylab = "Y,Yp")
lines(as.vector(t(Yp)), type = 'l', col = 'black')
legend("bottomright", c("Predicted", "Actual"),
col = c("red","black"),
lty = c(1,1), lwd = c(1,1))
x=data_cleaned[,1]
y=data_cleaned[,2]
head(x)
head(y)
library("mxnet")
data(iris)
x = iris[1:5!=5,-5]
y = as.integer(iris$Species)[1:5!=5]
train.x = data.matrix(x)
train.y = y
test.x = data.matrix(iris[1:5==5,-5])
test.y = as.integer(iris$Species)[1:5==5]
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=3, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
preds = predict(model, test.x)
pred.label = max.col(t(preds))
test.y
devtools::install_github("ropensci/tabulizer")
library(tabulizer)
library(mxnet)
data(lynx)
lynx
time(lynx)
length(lynx)
plot(lynx)
install.packages("zonator")
library(devtools)
devtools::install_github("cbig/zdat")
install.packages("trend")
data(maxau)
library(trend)
data(maxau)
head(maxau)
s <- maxau[,"s"]; Q <- maxau[,"Q"]
head(s)
head(Q)
partial.mk.test(s,Q)
data(Nile)
head(Nile)
plot(Nile)
res <- mk.test(Nile)
summary.trend.test(res)
res
smk.test(nottem)
head(nottem)
data(hcb)
plot(hcb)
sens.slope(s)
plot(s)
data("AirPassengers")
AirPassengers
class(AirPassengers)
frequency(AirPassengers)
plot(AirPassengers)
www="http://www.massey.ac.nz/~pscowper/ts/Maine.dat"
maine.month = read.table(www, header=TRUE)
www<-"http://www.massey.ac.nz/~pscowper/ts/Maine.dat"
Maine.month<-read.table(www, header=TRUE)
www<-"https://github.com/svkerr/R_Files/blob/master/TimeSeries/Maine.dat"
Maine.month<-read.table(www, header=TRUE)
library(astsa)
jj
time(jj)
cycle(jj)
plot(jj, ylab="Earnings per Share", main="J & J")
plot(jj, type="o", col="blue", lty="dashed")
plot(diff(log(jj)), main="logged and diffed")
plot(dog <- stl(log(jj), "per"))
plot(Nile)
plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3})",main="Annual River Nile Volume at Aswan, 1871-1970",type ="b")
Acf(beer2)
Acf(wineind)
library(forecast)
Acf(wineind)
Acf(beer2)
Pacf(wineind)
install.packages("Quandl")
library(Quandl)
gdp = Quandl("FRED/GDPC1",order="asc") # Download data via Quandl
head(gdp)
names(gdp) <- c("Time","GDP") # Rename variables
gdp[,"GDP"] <- log(gdp[,"GDP"]) # Take logs
library(ggplot2)
library(reshape2)
ggplot(gdp,aes(x=Time,y=GDP)) + geom_line(size=.5) + theme_classic() + labs(title="Log Real US GDP")
time.detrend <- residuals(lm(GDP ~ Time, data=gdp)) # Regress GDP on Time and obtain residuals
dat <- data.frame("Time"=gdp[,"Time"],"Linearly.Detrended"=time.detrend)
ggplot(dat,aes(x=Time,y=Linearly.Detrended)) + geom_hline(yintercept=0,colour="grey80") + geom_line(size=.5) + theme_classic() + labs(title="Linearly Detrended",y="")
adf.test(Linearly.Detrended))
adf.test(Linearly.Detrended)
library(astsa)
library(forecast)
library("tseries")
adf.test(Linearly.Detrended)
adf.test(dat$Linearly.Detrended)
data("JohnsonJohnson")
head(JohnsonJohnson)
gdp = Quandl("FRED/GDPC1",order="asc") # Download data via Quandl
head(gdp)
names(gdp) <- c("Time","GDP") # Rename variables
gdp[,"GDP"] <- log(gdp[,"GDP"]) # Take logs
ggplot(gdp,aes(x=Time,y=GDP)) + geom_line(size=.5) + theme_classic() + labs(title="Log Real US GDP")
time.detrend <- residuals(lm(GDP ~ Time, data=gdp)) # Regress GDP on Time and obtain residuals
dat <- data.frame("Time"=gdp[,"Time"],"Linearly.Detrended"=time.detrend)
head(dat)
ggplot(dat,aes(x=Time,y=Linearly.Detrended)) + geom_hline(yintercept=0,colour="grey80") + geom_line(size=.5) + theme_classic() + labs(title="Linearly Detrended",y="")
adf.test(dat$Linearly.Detrended)
head(gdp)
gdp = Quandl("FRED/GDPC1",order="asc") # Download data via Quandl
head(gdp)
names(gdp) <- c("Time","GDP") # Rename variables
head(gdp)
tail(gdp)
yearlymean <- ave(gdp$GDP, gl(71, 12), FUN = mean)
yearlymean
co2.dt <- gdp$GDP - yearlymean
plot(co2.dt)
adf.test(co2.dt)
head(gdp)
tail(gdp)
yearlymean <- ave(gdp$GDP, gl(71, 12), FUN = mean)
gdp.dt <- gdp$GDP - yearlymean
adf.test(gdp.dt)
data(lynx)
head(lynx)
fit <- nnetar(lynx)
fcast <- forecast(fit)
plot(fcast)
install.packages("nnfor")
data = read.csv('http://ucanalytics.com/blogs/wp-content/uploads/2015/06/Tractor-Sales.csv')
data = ts(data[,2],start = c(2003,1),frequency = 12)
plot(data, xlab='Years', ylab = 'Tractor Sales')
head(data)
plot(diff(data),ylab='Differenced Tractor Sales')
plot(log10(data),ylab='Log (Tractor Sales)')
plot(diff(log10(data)),ylab='Differenced Log (Tractor Sales)')
data=gtemp
plot(data, xlab='Years', ylab = 'Tractor Sales')
plot(data, xlab='Years', ylab = 'Global Temperature')
plot(diff(data),ylab='Differenced Global Temperature')
plot(log10(data),ylab='Log (Global Temperature)')
plot(diff(log10(data)),ylab='Differenced Log (Global Temperature)')
ARIMAfit = auto.arima(log10(data), approximation=FALSE,trace=FALSE)
data = read.csv('http://ucanalytics.com/blogs/wp-content/uploads/2015/06/Tractor-Sales.csv')
data = ts(data[,2],start = c(2003,1),frequency = 12)
head(data)
plot(data, xlab='Years', ylab = 'Tractor Sales')
plot(diff(data),ylab='Differenced Tractor Sales')
plot(log10(data),ylab='Log (Tractor Sales)')
plot(diff(log10(data)),ylab='Differenced Log (Tractor Sales)')
adf.test(diff(log10(data))
)
install.packages("FactorsR")
install.packages("fxregime")
library("fxregime")
library('strucchange')
data("FXRatesCHF", package = "fxregime")
data("FXRatesCHF")
head("FXRatesCHF")
cny = fxreturns("CNY", frequency = "daily",
start = as.Date("2005-07-25"), end = as.Date("2010-02-12"),
other = c("USD", "JPY", "EUR", "GBP"))
head(cny)
regx = fxregimes(CNY ~ USD + JPY + EUR + GBP,
data = cny, h = 100, breaks = 10, ic = "BIC")
summary(regx)
plot(regx)
round(coef(regx), digits = 3)
sqrt(coef(regx)[, "(Variance)"])
cit <- confint(regx, level = 0.9)
cit
breakdates(cit)
flm = fxlm(CNY ~ USD + JPY + EUR + GBP, data = cny)
scus = gefp(flm, fit = NULL)
plot(scus, functional = supLM(0.1))
lines(cit)
head(cny)
regx = fxregimes(CNY ~ USD + JPY + EUR + GBP,
data = cny, h = 100, breaks = 10, ic = "BIC")
summary(regx)
head(cny)
summary(regx)
plot(regx)
round(coef(regx), digits = 3)
sqrt(coef(regx)[, "(Variance)"])
cit <- confint(regx, level = 0.9)
cit
breakdates(cit)
flm = fxlm(CNY ~ USD + JPY + EUR + GBP, data = cny)
scus = gefp(flm, fit = NULL) #test for breakpoints
plot(scus, functional = supLM(0.1))
lines(cit)
library(strucchange)
library(astsa)
data("globtemp")
globtemp
plot(globtemp)
temp.cus = efp(globtemp ~ 1, type = "OLS-CUSUM")
summary(temp.cus)
plot(temp.cus, alpha = 0.01, boundary = FALSE)
bound <- boundary(temp.cus, alpha = 0.01)
lines(bound, col=4)
lines(-bound, col=4)
temp.cus
summary(lm(globtemp ~ 1))
plot(globtemp) #yearly time series
globtemp_win <- window(globtemp, end = 2000)
lev_fit <- lm(globtemp_win ~ 1)
summary(lev_fit)
plot(globtemp_win)
lines(ts(fitted(lev_fit), start = 1880, frequency = 1), col = 4)
globtemp_brk = breakpoints(globtemp_win ~ 1, h = 0.1)
summary(globtemp_brk)
plot(globtemp_win)
lines(fitted(globtemp_brk, breaks = 5), col = 4)
lines(confint(globtemp_brk, breaks = 5))
breakdates(globtemp_brk, breaks = 5)
library(quantmod)
library(ggplot2)
library(forecast)
library(tseries)
amzn.tkr = getSymbols("AAPL",from = as.Date("2017-01-04"), to = as.Date("2017-10-27"),auto.assign = F)
chartSeries(amzn.tkr)
a.tkr = getSymbols("AAPL",from = as.Date("2017-01-04"), to = as.Date("2017-10-27"),auto.assign = F)
a.tkr = getSymbols("AAPL",from = as.Date("2017-01-04"), to = as.Date("2017-10-27"),auto.assign = F)
chartSeries(a.tkr)
df = data.frame(date = index(a.tkr), a.tkr, row.names=NULL)
head(df)
ggplot(df, aes(df$date, df$AAPL.Adjusted)) + geom_line() + scale_x_date('Month/2017')  + ylab("AMZN Adjusted Stock Price") +
xlab("") + labs(title = "Apple Stock Price")
ggplot() +
geom_line(data = df, aes(x = df$date, y = df$AMZN.Adjusted, colour = "Daily Price")) +
geom_line(data = df, aes(x = df$date, y = df$ma7,   colour = "Weekly Moving Average"))  +
geom_line(data = df, aes(x = df$date, y = df$ma30, colour = "Monthly Moving Average"))  +
ylab('Apple Stock Price')
price_ma = ts(na.omit(df$ma7), frequency=30)
df$ma7 = ma(df$AAPL.Adjusted, order=7)
df$ma30 = ma(df$AAPL.Adjusted, order=30)
price_ma = ts(na.omit(df$ma7), frequency=30)
ggplot() +
geom_line(data = df, aes(x = df$date, y = df$AAPL.Adjusted, colour = "Daily Price")) +
geom_line(data = df, aes(x = df$date, y = df$ma7,   colour = "Weekly Moving Average"))  +
geom_line(data = df, aes(x = df$date, y = df$ma30, colour = "Monthly Moving Average"))  +
ylab('Apple Stock Price')
adf.test(price_ma, alternative = "stationary")
Acf(df$ma7, main='')
Pacf(df$ma7, main='')
deseasonal_ma <- seasadj(decomp)
decomp = stl(price_ma, s.window="periodic")
deseasonal_ma <- seasadj(decomp)
df_d1 = diff(deseasonal_ma, differences = 1)
plot(df_d1)
adf.test(df_d1, alternative = "stationary")
Acf(df_d1, main='ACF for Differenced Series')
Pacf(df_d1, main='PACF for Differenced Series')
fit_w_seasonality = auto.arima(deseasonal_ma, seasonal=TRUE)
fit_w_seasonality
seas_fcast = forecast(fit_w_seasonality, h=30)
plot(seas_fcast)
(bestfit <- list(aicc=fit_w_seasonality$aicc, i=0, j=0, fit=fit0))
(bestfit <- list(aicc=fit_w_seasonality$aicc, i=0, j=0, fit=fit_w_seasonality))
install.packages("DiagrammeR")
setwd("F:\\Forecasting_R\\1_Course\\Data\\section 6")
require(data.table)
require(TSA)
require(forecast)
train = fread("webtraf.csv")
head(train)
x = unlist(train[Page=="1984_(novela)_es.wikipedia.org_desktop_all-agents", -1])
x = tsclean(x)
head(x)
ndiffs(x)
p <- periodogram(x)
data.table(period=1/p$freq, spec=p$spec)[order(-spec)][1:2]
y = ts(x[1:490])
y.te = x[491:550]
fit0 = auto.arima(y)
(bestfit <- list(aicc=fit0$aicc, i=0, j=0, fit=fit0))
fc0 = forecast(fit0, h=60)
plot(fc0)
for(i in 1:3) {
for (j in 1:3){
z1 = fourier(ts(y, frequency=7.02), K=i)
z2 = fourier(ts(y, frequency=192), K=j)
fit = auto.arima(y, xreg=cbind(z1, z2), seasonal=F)
if(fit$aicc < bestfit$aicc) {
bestfit = list(aicc=fit$aicc, i=i, j=j, fit=fit)
}
}
}
bestfit
fc = forecast(bestfit$fit,
xreg=cbind(
fourier(ts(y, frequency=7.02), K=bestfit$i, h=60),
fourier(ts(y, frequency=192), K=bestfit$j, h=60)))
plot(fc)
